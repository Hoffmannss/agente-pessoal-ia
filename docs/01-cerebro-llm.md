# üß† FASE 1 ‚Äî O C√©rebro: LLM Local

> **Status:** üî¥ Pr√≥ximo passo | **Fase atual do projeto**

Este guia ensina como instalar, configurar e conectar um modelo de linguagem (LLM) rodando 100% no seu computador, sem depender de APIs externas pagas.

---

## Por que LLM Local?

| Aspecto | LLM Local (Ollama) | API Externa (OpenAI) |
|---|---|---|
| Custo | Gratuito ap√≥s hardware | Pago por token |
| Privacidade | 100% no seu PC | Dados saem para n√∫vem |
| Velocidade | Depende do hardware | Depende da internet |
| Controle | Total | Limitado |
| Offline | Sim | N√£o |

---

## Passo 1.1 ‚Äî Instalar o Ollama

Ollama √© o gerenciador de LLMs locais mais f√°cil de usar. Funciona como um "app store" de modelos de IA.

### Windows
```bash
# Op√ß√£o 1: Download direto
# Acesse: https://ollama.ai/download e baixe o instalador .exe

# Op√ß√£o 2: Via winget
winget install Ollama.Ollama
```

### Verificar instala√ß√£o
```bash
ollama --version
# Deve retornar algo como: ollama version 0.x.x

# Verificar se o servi√ßo est√° rodando
ollama list
```

---

## Passo 1.2 ‚Äî Escolher o Modelo Ideal

A escolha do modelo depende do seu hardware. Use esta tabela:

### Por Hardware (RAM)

| RAM Dispon√≠vel | Modelo Recomendado | Tamanho | Qualidade |
|---|---|---|---|
| 8 GB | `mistral:7b-instruct-q4_0` | ~4 GB | Boa |
| 8 GB | `llama3.2:3b` | ~2 GB | Boa |
| 16 GB | `mistral:7b-instruct` | ~7 GB | √ìtima |
| 16 GB | `llama3.1:8b` | ~8 GB | √ìtima |
| 32 GB | `mixtral:8x7b` | ~26 GB | Excelente |
| 32 GB+ | `llama3.1:70b-q4` | ~40 GB | Elite |

### Modelos com GPU (NVIDIA)

| VRAM | Modelo | Observa√ß√£o |
|---|---|---|
| 6 GB | `mistral:7b-q4` | Bom custo-benef√≠cio |
| 8 GB | `llama3.1:8b` | Recomendado |
| 12 GB | `mistral:7b` + `llama3.1:8b` | Qualidade alta |
| 24 GB | `mixtral:8x7b` | Profissional |

### Recomenda√ß√£o para Agente Pessoal

**Se voc√™ tem 16 GB de RAM ou 8 GB VRAM:**
```
Modelo principal: llama3.1:8b
Modelo r√°pido (tarefas simples): llama3.2:3b
```

**Se voc√™ tem 32 GB de RAM ou 16 GB VRAM:**
```
Modelo principal: mixtral:8x7b OU llama3.1:70b-q4
```

---

## Passo 1.2b ‚Äî Baixar o Modelo

```bash
# Baixar modelo recomendado (substitua pelo modelo escolhido)
ollama pull llama3.1:8b

# OU para PCs menos potentes:
ollama pull mistral:7b-instruct-q4_0

# OU o mais leve para testes:
ollama pull llama3.2:3b

# Ver modelos instalados:
ollama list

# Testar o modelo direto no terminal:
ollama run llama3.1:8b
# Digite sua mensagem e pressione Enter
# Ctrl+D para sair
```

---

## Passo 1.3 ‚Äî Conectar Ollama ao OpenClaw

### Via Interface do OpenClaw

1. Abra o OpenClaw no navegador (geralmente `http://localhost:3000`)
2. V√° em **Settings** (Configura√ß√µes)
3. Clique em **Credentials** ou **Connections**
4. Clique em **+ Add Credential**
5. Selecione **Ollama** como provider
6. Preencha:
   - **URL:** `http://localhost:11434`
   - **Nome:** `ollama-local`
7. Clique em **Test Connection** ‚Äî deve aparecer `Connected`
8. Salve

### Configurar o Agente para usar o Ollama

1. No OpenClaw, v√° ao seu agente (ou crie um novo)
2. Em **AI Model**, selecione **Ollama**
3. Escolha o modelo baixado (ex: `llama3.1:8b`)
4. Salve as configura√ß√µes

### Verificar se est√° funcionando
```bash
# No terminal, verifique se o Ollama est√° respondendo:
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.1:8b",
  "prompt": "Ol√°, tudo bem?",
  "stream": false
}'
```

---

## Passo 1.4 ‚Äî Testar via Telegram

1. Abra seu Telegram
2. Envie uma mensagem para o seu bot
3. O bot deve responder usando o LLM local
4. Verifique o tempo de resposta ‚Äî primeiras respostas podem ser lentas (modelo carregando)

**Mensagens de teste sugeridas:**
- "Ol√°, quem √© voc√™?"
- "Qual a capital do Brasil?"
- "Fa√ßa um resumo r√°pido sobre produtividade"

---

## Passo 1.5 ‚Äî Ajustar Par√¢metros

### Par√¢metros importantes no OpenClaw

```json
{
  "temperature": 0.7,
  "top_p": 0.9,
  "max_tokens": 2048,
  "context_window": 8192
}
```

| Par√¢metro | Valor Baixo | Valor Alto | Recomendado para Agente |
|---|---|---|---|
| `temperature` | Respostas previs√≠veis | Respostas criativas | 0.6 ‚Äì 0.8 |
| `top_p` | Mais focado | Mais diverso | 0.9 |
| `max_tokens` | Respostas curtas | Respostas longas | 1024 ‚Äì 2048 |
| `context_window` | Mem√≥ria curta | Mem√≥ria longa | 8192+ |

### System Prompt Inicial (Tempor√°rio)

Enquanto n√£o criamos o System Prompt completo na Fase 4, use este como ponto de partida:

```
Voc√™ √© um assistente pessoal inteligente e proativo. 
Voc√™ fala sempre em portugu√™s do Brasil.
Voc√™ √© direto, objetivo e √∫til.
Voc√™ se chama [NOME DO AGENTE - definir na Fase 4].
O usu√°rio se chama Hoffmannss.
Ajude sempre com clareza e precis√£o.
```

---

## Problemas Comuns e Solu√ß√µes

### Ollama n√£o responde
```bash
# Iniciar o servi√ßo manualmente:
ollama serve

# Verificar se a porta est√° em uso:
netstat -ano | findstr :11434
```

### Resposta muito lenta
- Verifique se o modelo cabe na sua RAM/VRAM
- Use uma vers√£o quantizada (ex: `q4_0` ao inv√©s de `q8`)
- Feche outros programas pesados

### OpenClaw n√£o conecta ao Ollama
- Confirme que o Ollama est√° rodando: `ollama list`
- Verifique o firewall do Windows
- URL correta: `http://localhost:11434` (n√£o `https`)

---

## Checklist de Conclus√£o da Fase 1

- [ ] Ollama instalado e rodando
- [ ] Modelo baixado e testado no terminal
- [ ] Ollama conectado ao OpenClaw
- [ ] Bot do Telegram respondendo com LLM local
- [ ] Par√¢metros ajustados
- [ ] System prompt tempor√°rio configurado

Quando todos estiverem marcados ‚úÖ, voc√™ est√° pronto para a **Fase 2: Mem√≥ria do Agente**!

---

## Recursos Adicionais

- [Site oficial do Ollama](https://ollama.ai)
- [Lista completa de modelos Ollama](https://ollama.ai/library)
- [Comparativo de modelos - Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
- [Documenta√ß√£o OpenClaw - Credentials](https://docs.openclaw.io)

---

*Pr√≥ximo: [Fase 2 ‚Äî Mem√≥ria do Agente](02-memoria.md)*
